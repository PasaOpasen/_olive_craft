{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize ONNX Models Throughput with OLive Python Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demos how to use OLive python package to optimize an ONNX model inference throughput performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install OLive package with command `pip install onnxruntime_olive==0.5.0 --extra-index-url https://olivewheels.azureedge.net/oaas` \n",
    "\n",
    "\n",
    "2. Install ONNX Runtime with \n",
    "\n",
    "    `pip install --extra-index-url https://olivewheels.azureedge.net/oaas onnxruntime_openvino_dnnl==1.11.0` for cpu\n",
    "    \n",
    "    or \n",
    "    \n",
    "    `pip install --extra-index-url https://olivewheels.azureedge.net/oaas onnxruntime_gpu_tensorrt==1.11.0` for gpu\n",
    "\n",
    "\n",
    "3. Install mlperf_loadgen with \n",
    "\n",
    "    `pip install --extra-index-url https://olivewheels.azureedge.net/oaas mlperf_loadgen`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Optimize ONNX Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurations for OLive Model Optimization includes:\n",
    "\n",
    "| Configuration  | Detail  | Example |\n",
    "|:--|:--|:--|\n",
    "| **model_path** | model path for optimization | \"PytorchBertSquad.onnx\" |\n",
    "| **openmp_enabled** | whether the onnxruntime package is built with OpenMP | False |\n",
    "| **result_path** | result directory for OLive optimization | \"olive_opt_result\" |\n",
    "| **throughput_tuning_enabled** | whether tune model for optimal throughput | True |\n",
    "| **max_latency_percentile** | throughput max latency pct tile | 0.95 |\n",
    "| **max_latency_ms** | max latency in pct tile in millisecond | 50 |\n",
    "| **dynamic_batching_size** | max batchsize for dynamic batching | 1 |\n",
    "| **threads_num** | threads num for throughput optimization | 4 |\n",
    "| **min_duration_sec** | minimum duration for each run in second | 10 |\n",
    "| **inputs_spec** | dict of inputâ€™s names and shapes | {\"attention_mask\": [1, 7], \"input_ids\": [1, 7], \"token_type_ids\": [1, 7]} |\n",
    "| **output_names** | output names for onnxruntime session inference | \"scores\" |\n",
    "| **providers_list** | providers used for perftuning | [\"cpu\", \"dnnl\"] |\n",
    "| **trt_fp16_enabled** | whether enable fp16 mode for TensorRT | True |\n",
    "| **quantization_enabled** | whether enable the quantization or not | True |\n",
    "| **transformer_enabled** | whether enable transformer optimization | True |\n",
    "| **transformer_args** | onnxruntime transformer optimizer args | \"--model_type bert\" |\n",
    "| **sample_input_data_path** | path to sample_input_data.npz | \"sample_input_data.npz\" |\n",
    "| **concurrency_num** | tuning process concurrency number | 2 |\n",
    "| **kmp_affinity** | bind OpenMP* threads to physical processing units | [\"respect,none\"] |\n",
    "| **omp_max_active_levels** | maximum number of nested active parallel regions | [\"1\"] |\n",
    "| **inter_thread_num_list** | list of inter thread number for perftuning | [1,2,4] |\n",
    "| **intra_thread_num_list** | list of intra thread number for perftuning | [1,2,4] |\n",
    "| **execution_mode_list** | list of execution mode for perftuning | [\"parallel\", \"sequential\"] |\n",
    "| **ort_opt_level_list** | onnxruntime optimization level | [\"all\"] |\n",
    "| **omp_wait_policy_list** | list of OpenMP wait policy for perftuning | [\"active\"] |\n",
    "| **warmup_num** | warmup times for latency measurement | 20 |\n",
    "| **test_num** | repeat test times for latency measurement | 200 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from olive.optimization_config import OptimizationConfig\n",
    "from olive.optimize import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-27 03:28:25,947 - olive.optimization_config - INFO - Checking the model file...\n",
      "2022-12-27 03:28:26,402 - olive.optimization_config - INFO - Providers will be tested for optimization: ['CPUExecutionProvider']\n",
      "2022-12-27 03:29:33,718 - olive.optimization.optimize_quantization - INFO - ONNX model quantization started\n",
      "2022-12-27 03:29:34,948 - root - INFO - Quantization parameters for tensor:\"input\" not specified\n",
      "2022-12-27 03:29:34,950 - root - INFO - Quantization parameters for tensor:\"157\" not specified\n",
      "2022-12-27 03:29:34,955 - root - INFO - Quantization parameters for tensor:\"161\" not specified\n",
      "2022-12-27 03:29:34,965 - root - INFO - Quantization parameters for tensor:\"164\" not specified\n",
      "2022-12-27 03:29:34,982 - root - INFO - Quantization parameters for tensor:\"168\" not specified\n",
      "2022-12-27 03:29:35,017 - root - INFO - Quantization parameters for tensor:\"171\" not specified\n",
      "2022-12-27 03:29:35,085 - root - INFO - Quantization parameters for tensor:\"174\" not specified\n",
      "2022-12-27 03:29:35,153 - root - INFO - Quantization parameters for tensor:\"178\" not specified\n",
      "2022-12-27 03:29:35,287 - root - INFO - Quantization parameters for tensor:\"181\" not specified\n",
      "2022-12-27 03:29:35,553 - root - INFO - Quantization parameters for tensor:\"184\" not specified\n",
      "2022-12-27 03:29:35,818 - root - INFO - Quantization parameters for tensor:\"188\" not specified\n",
      "2022-12-27 03:29:36,084 - root - INFO - Quantization parameters for tensor:\"191\" not specified\n",
      "2022-12-27 03:29:36,348 - root - INFO - Quantization parameters for tensor:\"194\" not specified\n",
      "2022-12-27 03:29:36,877 - root - INFO - Quantization parameters for tensor:\"195\" not specified\n",
      "2022-12-27 03:29:36,996 - root - INFO - Quantization parameters for tensor:\"197\" not specified\n",
      "2022-12-27 03:29:37,085 - root - INFO - Quantization parameters for tensor:\"200\" not specified\n",
      "2022-12-27 03:29:37,219 - root - INFO - Quantization parameters for tensor:\"223\" not specified\n",
      "2022-12-27 03:29:37,241 - root - INFO - Quantization parameters for tensor:\"226\" not specified\n",
      "2022-12-27 03:29:37,276 - root - INFO - Quantization parameters for tensor:\"249\" not specified\n",
      "2022-12-27 03:29:37,283 - root - INFO - Quantization parameters for tensor:\"252\" not specified\n",
      "2022-12-27 03:29:37,292 - root - INFO - Quantization parameters for tensor:\"275\" not specified\n",
      "2022-12-27 03:29:37,295 - root - INFO - Quantization parameters for tensor:\"278\" not specified\n",
      "2022-12-27 03:29:37,299 - root - INFO - Quantization parameters for tensor:\"output2\" not specified\n",
      "2022-12-27 03:29:37,301 - root - INFO - Quantization parameters for tensor:\"283\" not specified\n",
      "2022-12-27 03:29:37,304 - root - INFO - Quantization parameters for tensor:\"285\" not specified\n",
      "2022-12-27 03:29:37,305 - root - INFO - Quantization parameters for tensor:\"287\" not specified\n",
      "2022-12-27 03:29:37,307 - root - INFO - Quantization parameters for tensor:\"289\" not specified\n",
      "2022-12-27 03:29:37,482 - olive.optimization.optimize_quantization - INFO - Quantization optimization failed with error [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for ConvInteger(10) node with name 'Conv_0_quant'. Original model will be used for optimization.\n",
      "2022-12-27 03:42:37,858 - olive.optimize - INFO - Optimization succeeded, OLive tuning result written in v2\\olive_result.json\n"
     ]
    }
   ],
   "source": [
    "opt_config = OptimizationConfig(\n",
    "\n",
    "    model_path = \"./craft.onnx\",\n",
    "    sample_input_data_path=\"./input.npz\",\n",
    "    result_path = \"v2\",\n",
    "    #inputs_spec = {\"input\": [1, 3, -1, -1]},  # does not need if sample_input_data_path\n",
    "\n",
    "    dynamic_batching_size=1,\n",
    "\n",
    "    throughput_tuning_enabled=False,  # crashes on True\n",
    "    threads_num = 1,\n",
    "    max_latency_percentile = 0.95,\n",
    "    min_duration_sec=50,\n",
    "\n",
    "    openmp_enabled=False,\n",
    "    max_latency_ms = 1000,\n",
    "    \n",
    "    providers_list = [\n",
    "        \"cpu\", \n",
    "        # \"dnnl\"\n",
    "    ],\n",
    "    inter_thread_num_list=[1],\n",
    "    intra_thread_num_list=[1],\n",
    "    execution_mode_list = [\"sequential\"],\n",
    "    ort_opt_level_list=['all'],\n",
    "    quantization_enabled=True,\n",
    "\n",
    "    concurrency_num=1,  # uses 6 cores (from 12) with num=1, 10-12 cores with num=2 \n",
    "\n",
    "    warmup_num = 4,\n",
    "    test_num = 4\n",
    ")\n",
    "\n",
    "result = optimize(opt_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aprbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "48e09b1679f0fa91bfaf6ac40af5e9ef81e307a442fcfc8b241e3a12348c0001"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
